# Amazon Reviews Sentiment Analysis

This project performs sentiment analysis on Amazon product reviews using machine learning techniques. It classifies reviews as positive or negative based on the text content and explores patterns in consumer sentiment.

## Project Overview

- Data sourcing and exploration of Amazon reviews
- Text preprocessing and feature extraction
- Implementation of sentiment classification models
- Performance evaluation and visualization
- Analysis of model predictions and insights

## Stuff Used

- Python, Pandas, NumPy
- NLTK, scikit-learn
- Matplotlib, Seaborn
- Jupyter Notebooks

## Project Structure (Some of these folders are generated by the code/you should create)

- `data/`: Contains the dataset and processed files
- `notebooks/`: Jupyter notebooks for exploration and analysis
- `src/`: Source code for the project
- `models/`: Saved model files
- `visualizations/`: Generated charts and visualizations

## Done And Available
- text processing, training, prediction, etc... (pretty much everything in the src folder)
- exploratory data analysis jupyter notebook

## To-Do
- Jupyter notebook 2 (actual model comparisons)

## Results

Based on our exploratory data analysis, here are the key findings:

1. **Dataset Balance**: Our dataset contains a balanced distribution of positive and negative reviews.

2. **Text Length**: There appears to be a difference in text length between positive and negative reviews. In the chart for Review Lengths by Sentiment, we can see that theres a substantial peak at the sub 200 length section for negative reviews compared to positive reviews, suggesting customers may provide more brief explanations when expressing dissatisfaction.

3. **Common Words**: The most frequent words in positive reviews include "book", "one", "great", "movie", "good", "like", and "read", while negative reviews frequently contain similar words like "book", "one", "like", "would", "movie", "good", and "read". This interesting overlap suggests the dataset likely contains many book and movie reviews, with the sentiment often conveyed more through context and phrasing than specific positive or negative indicator words.

4. **Bigrams**: The most common phrase patterns in positive reviews include: "one best", "read book", "highly recommend", "year old", "would recommend", and "great cd". These patterns suggest strong recommendations, positive experiences, and frequently refer to gifts or suitable items for certain age groups. In negative reviews, the top bigrams are: "even though", "waste money", "much better", "thought would", "read book", "would recommend", and "could get". These phrases tend to convey disappointment, comparisons, and unmet expectations.

5. **Text Features**: 
- Exclamation marks: Positive reviews tend to contain more exclamation marks on average than negative ones. This suggests that positive sentiment may be expressed with more enthusiasm or excitement, whereas negative reviews are generally more reserved in punctuation.

- Question marks: Both sentiments rarely use question marks, but there is a slightly higher count in negative reviews. This could indicate that negative reviewers are more likely to express confusion, disappointment, or rhetorical questions.

- Uppercase ratio: There is no strong difference between positive and negative reviews in terms of the ratio of uppercase letters. However, both groups contain a few outliers with high uppercase usage, potentially reflecting emotional emphasis (e.g., shouting or emphasis like “DO NOT BUY”).

6. **Title-Review Relationship**: There is a weak positive correlation (correlation coefficient ≈ 0.205) between title length and review length. This suggests that, on average, longer titles tend to accompany longer reviews, but the relationship is not particularly strong. The scatterplot confirms this, showing a wide spread of review lengths across all title lengths. Additionally, both positive and negative reviews are distributed similarly, indicating that title length is not a strong differentiator of sentiment or review depth.

7. **Correlation Analysis**: Among the features analyzed, review length and review word count show the strongest (negative) correlations with sentiment, both around –0.12, suggesting that longer reviews are slightly more likely to be negative. Interestingly, question mark count also has a weak negative correlation (–0.13) with sentiment, reinforcing earlier observations that questions may signal dissatisfaction or confusion. Other features like exclamation count, uppercase ratio, and title characteristics have only very weak correlations (mostly between –0.05 and –0.01), indicating they may not individually carry strong predictive power but could still contribute when used in combination with other features during modeling.
